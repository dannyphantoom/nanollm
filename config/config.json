{
  "vocab_size": 32000,
  "dim": 768,
  "n_heads": 12,
  "n_layers": 12,
  "max_seq_len": 2048,
  "dropout": 0.1,
  "batch_size": 8,
  "learning_rate": 0.0003,
  "weight_decay": 0.1,
  "warmup_steps": 1000,
  "seq_len": 512,
  "epochs": 10,
  "grad_clip": 1.0,
  "training": {
    "checkpoint_dir": "checkpoints",
    "checkpoint_freq": 1,
    "log_wandb": true,
    "wandb_project": "nanollm",
    "eval_freq": 1000,
    "save_freq": 5000,
    "gradient_accumulation_steps": 4
  },
  "tokenizer": {
    "special_tokens": ["<pad>", "<unk>", "<bos>", "<eos>"],
    "min_frequency": 2
  },
  "optimizer": {
    "type": "adamw",
    "beta1": 0.9,
    "beta2": 0.95,
    "eps": 1e-8
  },
  "scheduler": {
    "type": "cosine",
    "min_lr": 1e-5,
    "warmup_ratio": 0.01
  }
}

